{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference for tricky symbols\n",
    "[Tricky symbols](http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html)\n",
    "\n",
    "[Table Generator](http://www.tablesgenerator.com/)\n",
    "\n",
    "[Fonts in Latex](https://www.sharelatex.com/learn/Mathematical_fonts)\n",
    "\n",
    "[Integrals, sums and limits](https://www.sharelatex.com/learn/Integrals,_sums_and_limits)\n",
    "\n",
    "$\\mathbb{R}$ = `$\\mathbb{R}$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure\n",
    "\n",
    "### Some general rules\n",
    "\n",
    "We use square brackets for layers, and round brackets for training examples.\n",
    "\n",
    "Training examples are spaced out horizontally as column vectors - i.e. the second value in the matrix shape $(n_x, m)$ generally corresponds to the number of training examples.\n",
    "\n",
    "### The general matrices\n",
    "\n",
    "We have 6 key matrices that we'll play with, which we can divide into two groups.\n",
    "\n",
    "The first four are representations of our features:\n",
    "* $X$, representing the input to our model - the training data. This has the shape $(n_x, m)$ where $n_x$ is the number of features, and $m$ is the number of training samples.\n",
    "* $Y$, representing the output to our model - the model outputs. This may have a number of shapes, depending on on the model structure we're chasing, but we can generally describe it as $(n_{x\\ final}, m)$ where $n_{x\\ final}$ is the number of neurons in our final (output) layer and $m$ is the number of training samples.\n",
    "* $Z$, representing the inputs to a layer that has undergone a linear transformation, shape $(n_x, m)$, where $n_x$ is the number of neurons on a given layer.\n",
    "* $A$, representing the activated linearised inputs, shape $(n_x, m)$, where $n_x$ is the number of neurons on a given layer and $m$ is the number of samples. This is essentially $Z$ but transformed.\n",
    "\n",
    "The second group of two are the parameters of our model:\n",
    "* $W$, representing the weights of the model. This is of shape $(n_x, n_{x-1})$, where $n_x$ and $n_{x-1}$ are the number of neurons in the current and previous layers, respectively.\n",
    "* $B$, representing the bias for each layer of the model. This is of shape $(n_x, 1)$, where $n_x$ is the number of neurons on the relevant layer. The second dimension is 1 because this is a bias relevant to each neuron.\n",
    "\n",
    "So given this:\n",
    "\n",
    "$X$ = $\\begin{bmatrix}\n",
    "    \\vdots & \\vdots & \\vdots &        & \\vdots \\\\\n",
    "    x^{(1)}  & x^{(2)}  & x^{(3)}  & \\dots  & x^{(m)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots &        & \\vdots \\\\\n",
    "\\end{bmatrix}$ = sum of feature vectors\n",
    "\n",
    "We would have \n",
    "\n",
    "$Z^{[1]} = W^{[1]}X^{[0]} + B^{[1]}$\n",
    "\n",
    "and \n",
    "\n",
    "$A^{[1]} = {\\sigma}(Z^{[1]})$\n",
    "\n",
    "\n",
    "### Some special cases\n",
    "\n",
    "Some simple cases:\n",
    "\n",
    "$A^{[0]} = X$, i.e. the 'activated output' of the zeroth layer is the input to the model.\n",
    "\n",
    "$A^{[n]} = \\hat{Y}$, i.e. the output of the last activation layer in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "$dz^{[l]} = d^{[l]}\\times g^{[l]\\prime}(z^{[l]})$, or\n",
    "\n",
    "$dz^{[l]} = W^{[l+1]T}dz^{[l+1]}\\times g^{[l]\\prime}(z^{[l]})$\n",
    "\n",
    "$dW^{[l]} = dz^{[l]}.a^{[l-1]}$\n",
    "\n",
    "$db^{[l]} = dz^{[l]}$\n",
    "\n",
    "$da^{[l-1]} = W^{[l]T}.dz^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "$(x,y)$ = training set, where $x\\in\\mathbb{R}^{n_x}, y\\in{0,1}$\n",
    "\n",
    "$(x^1, y^1)$ = single training example\n",
    "\n",
    "$(x^{\\{1\\}}, y^{\\{1\\}})$ = single mini-batch extracted from a full training set\n",
    "\n",
    "$x$ = feature vector\n",
    "\n",
    "$X$ = $\\begin{bmatrix}\n",
    "    \\vdots & \\vdots & \\vdots &        & \\vdots \\\\\n",
    "    x^{1}  & x^{2}  & x^{3}  & \\dots  & x^{m} \\\\\n",
    "    \\vdots & \\vdots & \\vdots &        & \\vdots \\\\\n",
    "\\end{bmatrix}$ = sum of feature vectors\n",
    "\n",
    "Note that we have $m$ samples as columns and $n_x$ features as rows. In Python, this comes out as `X.shape=(n_x, m)`.\n",
    "\n",
    "$Y$ = $\\begin{bmatrix} y^1 & y^2 & y^3 & \\dots & y^m \\end{bmatrix}$ = sum of labels. \n",
    "\n",
    "In Python, this comes out as `Y.shape=(1,m)`.\n",
    "\n",
    "$m$ = number of sample pairs\n",
    "\n",
    "$m_{train}$ = number of sample pairs in train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
