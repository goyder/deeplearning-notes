{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief summary\n",
    "\n",
    "Typically a framework for control problems.\n",
    "\n",
    "You have an **agent** and an **environment**. The agent **acts**, **observes**, and gets a reward or penalty, and updates accordingly.\n",
    "\n",
    "In brief:\n",
    "1. Observe\n",
    "2. Select action using policy\n",
    "3. Takes action\n",
    "4. Gets reward or penalty\n",
    "5. Update policy (learning step)\n",
    "\n",
    "[Ye old Wikipedia link.](https://en.wikipedia.org/wiki/Reinforcement_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Philosophically speaking...\n",
    "\n",
    "An interesting topic in this field is [Coherent Extrapolated Volition](https://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical challenges\n",
    "\n",
    "[A thorough blog post](http://amid.fish/reproducing-deep-rl) that covers some of the challenges of attempting to reproduce a reinforcement learning papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Capture the Flag\n",
    "[Blog post](https://deepmind.com/blog/capture-the-flag/) - a post from DeepMind, highlighting performance in Capture the Flag (from Quake 3).\n",
    "\n",
    "Key points, as described by Sean Driver at the Fast.ai session:\n",
    "* The map is procedurally generated - the AI is not learning the map, it's learning how to play the game.\n",
    "* It's playing as a team, against a team of components.\n",
    "* It developed standard techniques of CTF (e.g. working with teammates, working with a home-base)\n",
    "* It adapted to work with humans, *not* just humans.\n",
    "* Supposedly, the bots were ranked as friendlier than humans!\n",
    "\n",
    "### Dota 2\n",
    "\n",
    "[Link](https://qz.com/1052409/openai-just-beat-a-professional-dota-2-player-at-the-international-2017/) - a bot was trained against itself to play 1v1 Dota 2. In a showcase, it was able to beat Dendi, one of the top Dota 2 players.\n",
    "\n",
    "### Sonic the Hedgehog\n",
    "\n",
    "[An example solution](https://github.com/wassname/world-models-sonic-pytorch).\n",
    "\n",
    "One of the winning methods for the Sonic the Hedgehog bot was to simply \"wallhack\" - it eventually found and utilised bugs to allow it warp through half the level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons\n",
    "* It's not sample efficient\n",
    "* It cheats - grey goo problems\n",
    "* [Doesn't work yet.](https://alexirpan.com/2018/02/14/rl-hard.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry applications:\n",
    "\n",
    "* Paul Wighton of [3dimageautomation](3dimageautomation.com.au) is using this in industry.\n",
    "* Prof. Pieter Abbeel is quitting his job to follow it.\n",
    "* Spica.ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Related fields\n",
    "\n",
    "When tying it to supervised and unsupervised: Yann Lecun's cake.\n",
    "\n",
    "Very cross-skilled. Related fields might include:\n",
    "* Bounded rationality\n",
    "* Machine learning\n",
    "* Operations research\n",
    "* Conditioning \n",
    "* Reward systems\n",
    "* Optimal control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related concept: World models\n",
    "\n",
    "Broad concept: \"Wouldn't it be good if the system had some idea about the physics or how the world works before your model took action?\"\n",
    "\n",
    "Controller:\n",
    "\n",
    "* \"RL Algorithms landscape\" - one of use is proximal policy optmisation\n",
    "\n",
    "Subconcepts:\n",
    "* Variable auto-encoders - \"Imagine if someone had to recreate a drawing from a single sentence. What concepts would you choose to write down or encode?\" [A brief introduction.](http://kvfrans.com/variational-autoencoders-explained/)\n",
    "* MDN-RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related concept: Mixture density networks\n",
    "\n",
    "[A quick link.](http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html)\n",
    "\n",
    "Broad concept: You might have some system - the thing I'm trying to predict isn't gaussian, but maybe it's two (or ten) gaussian models with peaks... (I still have no idea what this means)\n",
    "\n",
    "Related: sketch RNN. Hardmaru released a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links and papers\n",
    "\n",
    "[Machine efficient solution of poker.](https://arxiv.org/pdf/1805.09195.pdf)\n",
    "\n",
    "[Style transfer in NLP.](https://t.co/Y9VzvGOHWk) Recently out of Stanford. Previously not done for text.\n",
    "\n",
    "[MDN tutorial](https://github.com/hardmaru/pytorch_notebooks/blob/master/mixture_density_networks.ipynb)\n",
    "\n",
    "Open AI Docker submissions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
