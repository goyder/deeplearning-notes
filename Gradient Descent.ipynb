{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent underpins a great deal of the world of deep learning.\n",
    "\n",
    "Some further links of interest:\n",
    "\n",
    "* [An overview of gradient descent optimization algorithms](ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundaments\n",
    "\n",
    "For a given cost function, $J(w)$:\n",
    "\n",
    "$Repeat\\, \\{ \\\\\n",
    "w := w - \\alpha\\frac{dJ(w, b)}{dw} \\\\\n",
    "b := b - \\alpha\\frac{dJ(w, b)}{db} \\\\\n",
    "\\}$\n",
    "\n",
    "In the context of code, we might abbreviate $\\frac{dJ(w, b)}{dw}$ as `dw` for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "Consider the case where we have two hidden layers, each with an activation function, with some input vector $X$ of dimension $(n_x, m)$. (This corresponds to a feature vector with $n_x$ features and $m$ samples.\n",
    "\n",
    "Hence, we define our forward propagation as:\n",
    "\n",
    "$Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "\n",
    "$A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "\n",
    "$A^{[2]} = g^{[2]}(Z^{[2]}) = \\sigma(Z^{[2]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "So now, we've arrived at some collection of outputs which give us an estimation of our classes, $Y$. Working backwards:\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y$\n",
    "\n",
    "We recalculate our weights and biases:\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[2]T}$\n",
    "\n",
    "$dB^{[2]} = \\frac{1}{m}{np.sum}(dZ^{[2]}, axis=1, keepdims=True)$\n",
    "\n",
    "And then step back over the previous layers:\n",
    "\n",
    "$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})$\n",
    "\n",
    "Again, recalculating our weights and biases:\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{[1]T}$\n",
    "\n",
    "$dB^{[1]} = \\frac{1}{m}{np.sum}(dZ^{[1]}, axis=1, keepdims=True)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of activation functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU and Leaky ReLU\n",
    "\n",
    "Here defined as: $g(z) = {max}(0, z)$, we can calculate the derivative $g'(z) in a straightforward manner:\n",
    "\n",
    "$g'(x) = \\begin{cases}\n",
    "0\\ if\\ z<0\\\\\n",
    "1\\ if\\ z\\ge0\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "Or, in the case of leaky ReLU, where $g(z) = {max}(0.01z, z)$,\n",
    "\n",
    "$g'(x) = \\begin{cases}\n",
    "0.01\\ if\\ z<0\\\\\n",
    "1\\ if\\ z\\ge0\\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "Where $g(z) = \\frac{1}{(1+e^{-z})}$,\n",
    "\n",
    "$g'(z) = \\frac{1}{(1+e^{-z})}(1-\\frac{1}{(1+e^{-z})})$\n",
    "\n",
    "or, put more simply:\n",
    "\n",
    "$g'(z) = g(z)(1-g(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "Where $g(z) = \\tanh(z)$,\n",
    "\n",
    "$g'(z) = 1 - \\tanh^2(z) = 1 - (g(z))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic vs Batch vs Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent runs gradient descent for each training sample. This is relatively ineffecient and may be too eager too eager to adjust model performance, and moreover, is too slow: We want to take advantage of vectorisation for our training.\n",
    "\n",
    "With batch gradient descent, we run the whole dataset through the model and adjust the parameters.\n",
    "\n",
    "In practise, if we have, say, 5,000,000 samples in our training sets, we might not like to wait until we've run all 5,000,000 samples to run gradient descent.\n",
    "\n",
    "Instead, we split our training set up into \"mini-batches\", and run one round of gradient descent for each \"mini-batch\".\n",
    "\n",
    "We call this \"mini-batch gradient descent\", in contrast to \"batch gradient descent\" (which refers to using the whole dataset.\n",
    "\n",
    "[Reference](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences in observing training\n",
    "As we're running over a different dataset each time, we might note that the cost function jumps around (but, generally speaking, trends downwards.)\n",
    "\n",
    "<img src=\"static/batch_vs_minibatch_training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing mini-batch size\n",
    "\n",
    "If $n_{minibatch} = m$: we've just got gradient descent.\n",
    "\n",
    "If $n_{minibatch} size = 1$: we've just got stochastic gradient descent.\n",
    "\n",
    "Typical mini-batch sizes are powers of 2 - 32, 64, 128, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponentially weighted averages\n",
    "\n",
    "For $n$ noisy, but related points, we might like to use the weight of the previous points to determine an average. One such approach is [exponentially weighted averages](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average), where\n",
    "\n",
    "$v_t = \\beta v_{t-1} + (1-\\beta) \\theta_t$\n",
    "\n",
    "We can think of $v_t$ as approximating $\\frac{1}{1-\\beta}$ days temperature, i.e. when $\\beta=0.9$, we are approximating 10 day's temperatures.\n",
    "\n",
    "#### Bias correction\n",
    "\n",
    "We don't want to start with 0 in our values - this may skew our first few results. So to correct for this initial bias, we might just take the first value as our initial value, or perhaps sample a few values.\n",
    "\n",
    "[A discussion is here.](https://blog.fugue88.ws/archives/2017-01/The-correct-way-to-start-an-Exponential-Moving-Average-EMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations and Flavours of Grad Descent\n",
    "\n",
    "A good summary of all these flavours are [here](http://ruder.io/optimizing-gradient-descent/index.html#adam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "This stands for \"root mean square\" prop.\n",
    "\n",
    "The key intuition of RMS prop is that we take an exponentially weighted average of the square of the $dW$ and $dB$ terms, and then update the values with our term divided by the square of the average. i.e.\n",
    "\n",
    "$S_{dW} = \\beta S_{dW} + (1-\\beta){dW}^2$\n",
    "\n",
    "$W := W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}}$\n",
    "\n",
    "The appeal of this is that the weighting and division is carried out for each dimension - we are essentially scaling the gradient change term based on the magnitude of the gradient change.\n",
    "\n",
    "In practise, we also add a small $\\epsilon$ value (e.g. $10^{-8}$) in the denominator to prevent the denominator from going too small and blowing up the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "The key intuition here is that it's a combination of RMSprop and Momentum approaches.\n",
    "\n",
    "So:\n",
    "\n",
    "$V_{dW} = \\beta_1 V_{dW} + (1-\\beta_1)dW$\n",
    "\n",
    "$S_{dW} = \\beta_2 S_{dW} + (1-\\beta_2)dW^2$\n",
    "\n",
    "$W := W - \\alpha \\frac{V_{dW}}{\\sqrt{dW}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
