{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "[Obligatory Wikipedia link.](https://en.wikipedia.org/wiki/Word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key element in the field of word embeddings is the *embedding matrix*.\n",
    "\n",
    "Say that we have a 10,000 word vocabulary, and we have an embedding size of 300. Our embedding matrix, $E$, will be a $300 \\times 10000$ matrix - or $\\text{shape}(E) = (300,10000)$.\n",
    "\n",
    "Each row in the embedding matrix refers to some kind of unknown \"quality\" - we let the system determine this itself. At an intuitive level, we say that each row might refer to some understandable quality - the \"gender\", \"age\", \"foodness\", \"city-ness\" of some word - but in the end, these embeddings are not specified or required to be any sort of human-understandable concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we tell what words are similar?\n",
    "\n",
    "There are a few approaches to determining whether words are similar.\n",
    "\n",
    "#### Euclidean distance\n",
    "\n",
    "One simple approach is to take the Euclidean distance.\n",
    "\n",
    "#### Cosine similarity\n",
    "\n",
    "Another approach is to take the Cosine similarity. In this case:\n",
    "\n",
    "$\\text{CosineSimilarity(u, v)} = \\frac{u . v}{ ||u||_2 ||v||_2 } = cos(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches \n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "A key approach is [`Word2Vec`](https://en.wikipedia.org/wiki/Word2vec). \n",
    "\n",
    "\n",
    "\n",
    "A good explanation is provided [here](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Of course, like any good algorithm, even though the underlying ideas of Word2Vec are surprisingly intuitive, there are many clever tricks to make it work. Some of these are fairly clearly explained in a sequel blog post, [here](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we visualise a 300-dimensional matrix?\n",
    "\n",
    "#### The simple answer:\n",
    "\n",
    "Picture an n-dimension matrix, and then let $n = 300$.\n",
    "\n",
    "#### But otherwise\n",
    "\n",
    "There are a number of visualisation tools available. \n",
    "\n",
    "##### t-SNE\n",
    "\n",
    "The classic is [`t-SNE`](https://lvdmaaten.github.io/tsne/), or \"t-Distributed Stochastic Neighbor Embedding\". This is a \"technique for dimensionality reduction that is particularly well suited for the visualisation of high-dimensional datasets\".\n",
    "\n",
    "There's a [techtalk here](https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw), and plenty of papers and supplementary material on the [website](https://lvdmaaten.github.io/tsne/).\n",
    "\n",
    "##### UMAP\n",
    "\n",
    "[UMAP](https://github.com/lmcinnes/umap), or \"Uniform Manifold Approximation and Project\", is getting some attention as another dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers\n",
    "\n",
    "* [2003 - \"A neural probabilistic language model\"](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
