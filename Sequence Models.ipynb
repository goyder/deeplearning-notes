{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are sequence data?\n",
    "\n",
    "Examples could include:\n",
    "* Audio data\n",
    "* Text (\"the quick brown fox jumped over the lazy dog\")\n",
    "* Music\n",
    "* DNA sequences (duh?)\n",
    "* A series of video frames\n",
    "\n",
    "[DNA sequences](https://en.wikipedia.org/wiki/Sequence_database) actually dominate the Google results if you are to search for \"sequence data\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider an example\n",
    "\n",
    "Let's try and determine which of the following words refer to a name.\n",
    "\n",
    "x: `Harry Potter and Hermione Granger invented a new spell.`\n",
    "\n",
    "x: [ $x^{<1>}\\ x^{<2>}\\ ...\\ x^{<t>}\\ ...\\ x^{<9>}$ ]\n",
    "\n",
    "y: [ 1 1 0 1 1 0 0 0 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General architecture\n",
    "\n",
    "The distinguishing feature of a basic recurrent neural network is that it feeds the outputs of a given value back into itself as an additional input for the next step.\n",
    "\n",
    "<img src=\"static/Recurrent_neural_network_unfold.svg.png\">\n",
    "*Source: [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg)*\n",
    "\n",
    "In essence, allows it to exhibit \"dynamic temporal behaviour along a sequence\" - or, in other words, what happens before or after in the sequence can impact the current result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some varieties?\n",
    "\n",
    "A key way to break down RNNs is by their inputs and outputs:\n",
    "* **One-to-one** - basically a standard neural network, barely an RNN\n",
    "* **One-to-many** - e.g. generation of a piece of music from a starting point\n",
    "* **Many-to-many** - e.g. determining whether a word in a sentence is a name\n",
    "    * Many-to-many can have two forms - either $T_x = T_y$ or $T_x \\neq T_y$. The latter might be used for translation, where you don't expect each word to have a one-to-one equivalent.\n",
    "* **Many-to-one** - e.g. sentiment analysis of a sentence\n",
    "\n",
    "<img src=\"static/rnntypes.png\">\n",
    "*Source: Andrew Ng's Sequence Models course*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we train them?\n",
    "\n",
    "This can be a bit more complicated than usual, given the recurrent nature. \n",
    "\n",
    "[A gentle introduction is provided here](https://machinelearningmastery.com/gentle-introduction-backpropagation-time/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradient problem\n",
    "\n",
    "As information passes along a network in forward and back prop, information is lost in transmission. For example, $\\hat{y}^{<3>}$ is strongly influenced by points immediately before and after it. This makes it challenging to capture long range dependencies. This issue was explored as early as [1991](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) (warning - paper in German) and [1994](http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf).\n",
    "\n",
    "In the world of RNNs, vanishing gradients tend to be the more common issue, but exploding gradients can be catastrophic. (This will show up as chains as `NaNs`, for instance, as you get numerical overflow.)\n",
    "\n",
    "##### A solution: the Gated Recurrent Unit.\n",
    "One of the approaches to address this are with the GRU principle, [explained here](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be).\n",
    "\n",
    "##### Another (more widely used solution): the Long Short Term Memory model.\n",
    "This model is actually over [20 years old](http://www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "##### But some people have done comparisons.\n",
    "\n",
    "[Here](http://arxiv.org/pdf/1503.04069.pdf) and [here](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
    "\n",
    "##### I don't get how these models work.\n",
    "\n",
    "First, follow [this](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  explanation of an LSTM - follow it intuitively.\n",
    "\n",
    "Make sure that you understand the concepts of the gates.\n",
    "\n",
    "*Then*, try to understand a GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What distinguishes these networks?\n",
    "\n",
    "#### In comparison to a standard network\n",
    "\n",
    "Inputs and outputs can be different lengths in different examples.\n",
    "\n",
    "A naive standard network will not share features learned across different positions of text.\n",
    "\n",
    "### What are their shortcomings?\n",
    "\n",
    "A key shortcoming is that in a typical model, only data prior to a current data point is available for prediction.\n",
    "\n",
    "Therefore, $\\hat{y^<n>}$ can only learn from $x^{<1>, <2>, ..., <n-1>}$.\n",
    "\n",
    "This can be addressed with \"bidirectional recurrent neural networks\", in comparison with the vanilla \"unidirectional recurrent neural networks\". (These have their own shortcomings - for instance, a BRNN needs the full sequence before it can predict.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A special case: Language modelling\n",
    "\n",
    "A language model estimates the 'probability' of a given sequence of words, i.e. $P(y^{<1>},y^{<2>},...,y^{<T_y>})$.\n",
    "\n",
    "In a well functioning model, we would want logical sentences of words to be ranked higher - e.g. $P(\\text{What is going on here}) = 3.2 \\times 10^{-13}$, and $P(\\text{Who the hecky's a what}) = 1 \\times 10^{-16}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you build one?\n",
    "\n",
    "1. Take a large corpus of English text.\n",
    "2. Tokenise it - map each word or component into a given example.\n",
    "   * You might like to include an End of Sentence token, sometimes marked as `<EOS>`.\n",
    "  * If you are using a limited vocabulary, you might like to replace \"unknown words\" with an \"unknown\" token, sometimes marked as `<UNK>`.\n",
    "3. We then build an RNN. \n",
    "  * At each step, the model attempts to guess what word comes next, given the activations of the previous step. \n",
    "  <img src=\"static/RNN model summary.png\">\n",
    "  * The loss function is defined as: $\\mathcal{L}(\\hat{y}^{<t>},y^{<t>})=-\\sum_{i}y_i^{<t>}\\log\\hat{y}^{<t>}$ - i.e. logistic regression.\n",
    "  \n",
    "When we want to start producing our own outputs, we provide a starting point (i.e. starting token, or tokens) and then start feeding the predictions into the next layer.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different approaches\n",
    "\n",
    "You may like to build it as a character or word level tokens.\n",
    "\n",
    "A character language model results in much longer sequences. \n",
    "* Advantageously, they don't need a limited vocabulary. You only have 26 tokens (plus punctuations). \n",
    "* Negatively, they tend not to be so good at capturing long-range dependencies and computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Generation\n",
    "\n",
    "The goal of caption generation is to accept an *image* as an input, and output a *text caption* as an output.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The general architecture is as follows:\n",
    "* Input an image.\n",
    "* Pass it through a convolutional network (e.g. AlexNet)\n",
    "* Take the feature vector from the dense network at the end of the model.\n",
    "* Pass it into an LSTM (or GRU, etc) and output a series of words based on this input.\n",
    "\n",
    "Many other options can be found in [A Gentle Introduction to Deep Learning Caption Generation Models](https://machinelearningmastery.com/deep-learning-caption-generation-models/).\n",
    "\n",
    "#### References\n",
    "\n",
    "* [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](https://arxiv.org/pdf/1412.6632)\n",
    "* [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf)\n",
    "* [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation\n",
    "\n",
    "The goal of caption generation is to accept an *sentence of text in language A* as an input, and output *the same sentence of text in language B* as an output.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The general architecture is as follows:\n",
    "* Pass in a sentence through an \"encoder\" network - one word at a time into an RNN.\n",
    "* This generates a \"sentence encoding\".\n",
    "* Pass this encoding into a \"decoding\" network.\n",
    "* Generate a new sentence from this decodering network.\n",
    "\n",
    "We are generating the probability of a given English sentence being the output, given an input foreign sentence.\n",
    "\n",
    "#### Beam search\n",
    "\n",
    "Beam search is a common algorithm for NLP.\n",
    "\n",
    "#### References\n",
    "* [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n",
    "* [Beam Search Strategies for Neural Machine Translation](https://arxiv.org/abs/1702.01806)\n",
    "* [Beam search](https://en.wikipedia.org/wiki/Beam_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
