{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are any number of 'tactics' that could be taken to 'improve' a machine learning model. These might include:\n",
    "* Collect more data\n",
    "* Collect a more diverse training set\n",
    "* Train the algorithm for longer\n",
    "* Try a different gradient descent algorithm\n",
    "* Try a bigger network\n",
    "* Try a smaller network\n",
    "* Try drop out\n",
    "* Add $L_2$ regularisation\n",
    "* Play with the network architecture.\n",
    "\n",
    "Nice head, and plenty of strategies, but how do we decide what comes next?\n",
    "\n",
    "Thus, the driving motivation of this notebook (and of course, Andrew Ng's course!):\n",
    "* Analyse your machine learning problem and determine what is to be done next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Orthogonalisation\n",
    "\n",
    "We want to understand a clear cause and effect when tuning. This is closely related to the concept of orthogonalisation.\n",
    "\n",
    "If our controls are 'orthogonal', then each is independent to the others. We want a single control to achieve a single action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of assumptions in machine learning\n",
    "\n",
    "When we train a model we:\n",
    "1. Fit the training set well on the cost function.\n",
    "2. Fit the dev set well on the the cost function.\n",
    "3. Fit the test set well on the cost function.\n",
    "4. Hope that the model performs well in the real world.\n",
    "\n",
    "Each of these has a distinct set of tools that we can apply to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for each step of the process\n",
    "\n",
    "We've outlined four steps in the above. Let's explore what tools might be appropriate at each step.\n",
    "\n",
    "1. **The model doesn't function well on the training data.** We might consider:\n",
    "    * Working with a bigger network\n",
    "    * Exploring different optimisation methods (tuning our parameters for Adam, for instance.)\n",
    "2. **The model doesn't function well on the dev data.** We might consider:\n",
    "    * Explore regularisation (dropout, $L_2$ regularisation, etc.)\n",
    "    * Get a bigger training set\n",
    "3. **The model doesn't function well on the test data.** We might consider:\n",
    "    * Get a bigger dev set\n",
    "4. **The model doesn't perform well in reality.** We might consider:\n",
    "    * Change the dev set\n",
    "    * Change the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on early stopping\n",
    "\n",
    "The concern with using early stopping is that it impacts on training and dev performance at the same time - it's not an 'orthogonal' tool. This is not the end of the world, but we might consider using other systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Single number evaluation metric\n",
    "\n",
    "Certainly, we generally struggle to find a single number that accurately describes how a model performs in all cases. This becomes even more important when we try to explain a model to a stakeholder, taking into account their biases and the like.\n",
    "\n",
    "All the same we generally need a quick and reliable way to establish whether a model is 'better' or not. \n",
    "\n",
    "**Enter the single number evaluation metric**. This lets us quickly compare model A against model B, so as to speed up the iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisficing and Optimising metrics\n",
    "\n",
    "Sometimes we may have concepts or concerns that do not define our model performance, per se, but may qualify or disqualify it. \n",
    "\n",
    "We could define our metrics into \n",
    "* 'optimising' metrics - metrics we strive to improve and judge upon\n",
    "* 'satisficing' metrics - metrics that simply qualify whether something is 'good enough' - similiar to the concept of 'hygiene' in other contexts\n",
    "\n",
    "You could phrase this as: \"Optimise for metric **X**, subject to requirements of metric **Y**.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to human level performance\n",
    "\n",
    "<img src=\"static/comparing_to_human_level.png\">\n",
    "\n",
    "As we train our model, we might find there are a few distinct sections.\n",
    "\n",
    "* Provided all is trucking along quite nicely, performance grows rapidly until we surpass human level performance.\n",
    "* As we surpass human level performance, the rate of improvement drops off.\n",
    "* The rate of improvement slows to zero as we approach [\"Bayes optimal error\"](https://en.wikipedia.org/wiki/Bayes_error_rate) - this is the irreducable error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimising for bias or variance\n",
    "\n",
    "Let's consider two scenarios: One where the human (or perhaps Bayes) error is 1.1%, and one where the human error is 7.5%.\n",
    "\n",
    "| Error type | Case 1 | Case 2|\n",
    "| --- | --- | --- |\n",
    "| Human | 1% | 7.5% | \n",
    "| Training | 8% | 8% |\n",
    "| Dev | 10% | 10% |\n",
    "\n",
    "Now, in Case 1, there's an enormous disparity between both the training and dev set and the human performance. This suggests that we want to clear our bias first, but improving the performance of the model.\n",
    "\n",
    "In Case 2, the difference between human and training is quite small, but there's a fair difference between training and dev. We should be focusing on varias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Dataset creation and distributions\n",
    "\n",
    "How the train, dev, and test sets are created are hugely important for the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO**: Choose a dev set and test set to reflect data **you expect to get in the future** and **consider important to do well on**.\n",
    "\n",
    "Don't just sample from different pools for the training, dev, and test datasets. It's tempting to segment an dataset based on clear 'fissures' in the data. \n",
    "\n",
    "The risk is that your training, dev, and test datasets end up marking completely different goals, and when you go to move your optimised model to a new dataset, it works not at all!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DO**: Set your test set to be big enough to give high confidence in the overall performance of your system. \n",
    "\n",
    "Split your dataset based on your training set sizes. \n",
    "\n",
    "If you're working the 'small dataset' size - i.e. perhaps up to 10,000 training samples - the old rule of 60/20/20 training/dev/test works well.\n",
    "\n",
    "If you're in the world of millions of examples, we might consider splitting it down into 98/1/1 for training/dev/test - this would still be leaving 10,000 samples for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with mismatched datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a relatively small amount of polished data that I want to reserve for the dev and test dataset. Instead, the majoirty of my training dataset will come from some other, easily obtainable bulk source, mixed with a reasonable amount of my polished data.\n",
    "\n",
    "To test this process, I may now add a new distribution - a \"training-dev\" set, which is a separate subset of the training set. By evaluating the error at each step, we can glean different aspects about performance:\n",
    "\n",
    "| Error type | Rate (for example) | If large error from previous row, may be due to... |\n",
    "| --- | --- | --- |\n",
    "| Human | 1% |  |\n",
    "| Training set | 2% | Avoidable bias |\n",
    "| Training-dev set | 3% | Variance in training set | \n",
    "| Dev | 4% | Data mis-match |\n",
    "| Test | 5% | Somehow overfit to dev set | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Error Analysis Procedure\n",
    "\n",
    "When we are focusing on how to improve our model, we would do well to:\n",
    "* Identify where the model struggles;\n",
    "* Note several paths that could be taken to improve performance;\n",
    "* Evaluate the potential upper bound in model performance improvement if we were to perfectly follow that evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
